# -*- coding: utf-8 -*-
"""AttributeInference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aDytnYLrOf0M2uUjlSNuIJ1iDwUolK3G
"""

# from google.colab import drive
# drive.mount('/content/drive')
import torch
import os
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from attributeDataset import AttributeDataset
from network import NeuralNetwork

device = "cpu"
global_path="D:/[Saarland]/[Semester1]/[PET]/[petProject]/AttributeInference/Project"
# global_path="drive/MyDrive/pet/AttributeInversion"

def get_data_loaders(batch_size_train, batch_size_test):
    # train_set = AttributeDataset(global_path+"/data/faces/training/")
    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    # ])
    train_set = AttributeDataset(global_path+"/data/UTKFace/Target/Train/")
    # train_set, val_set = torch.utils.data.random_split(dataset, [50000, 10000])

    train_load = DataLoader(
        dataset=train_set, batch_size=batch_size_train,  # shuffle=True,
        num_workers=8, pin_memory=True)
    test_set = AttributeDataset(global_path+"/data/UTKFace/Target/Test/")
    test_load = DataLoader(
        dataset=test_set, batch_size=batch_size_test,  # shuffle=True,
        num_workers=8, pin_memory=True)
    return train_load, test_load


def train(dataloader, model, loss_fn, optimizer, epochs, path=global_path+"/model_weights/model_inversion.pth",
          train_shallow=False):
    size = len(dataloader.dataset)
    model.train()
    for epoch in range(epochs):
        for batch, (X, y) in enumerate(dataloader):
            # Compute prediction and loss
            X= X.to(device)
            y= y.to(device)
            
            pred,embed = model(X.float())
            loss = loss_fn(pred, y)

            # Backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            if batch % 1 == 0:
                loss, current = loss.item(), batch * len(X)
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")

    print("training finished")
    torch.save(model.state_dict(), path)


def test(dataloader, model, path=global_path+"/model_weights/mnist_net.pth", shallow=False):
    model.load_state_dict(torch.load(path))
    correct = 0
    total = 0
    model.eval()
    with torch.no_grad():
        for data in dataloader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs,embed = model(images.float())
            if shallow:
                outputs = F.log_softmax(outputs)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print("Accuracy : ", 100 * correct / total)

if __name__ == "__main__":
    if not os.path.exists(global_path+"/model_weights"):
        os.mkdir(global_path+"/model_weights")
    num_classes=117
    train_loader, test_loader = get_data_loaders(128, 128)
    loss_fn = nn.CrossEntropyLoss()
    model = NeuralNetwork(num_classes,isTarget=True).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=0.0001)
    # model.load_state_dict(torch.load(global_path+'/model_weights/face_net.pth')
    # model.load_state_dict(torch.load(global_path+'/model_weights/face_net.pth',map_location=torch.device('cpu')))
    train(train_loader, model, loss_fn, optimizer, 200,
          path=global_path+"/model_weights/target_net.pth")
    test(test_loader, model, path=global_path+"/model_weights/target_net.pth")

